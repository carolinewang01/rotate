defaults:
  - hydra: hydra_simple
  - ego_defaults@ego_train_algorithm
  - heldout_eval
  - _self_

# ENV_NAME: lbf
# ROLLOUT_LENGTH: 128
# ENV_KWARGS: {}

ENV_NAME: overcooked-v1
ROLLOUT_LENGTH: 400
ENV_KWARGS: 
  layout: "cramped_room" # asymm_advantages, coord_ring, counter_circuit, cramped_room, forced_coord
  max_steps: 400

# name: brdiv
name: ${ENV_KWARGS.layout}/brdiv

# training settings
train_ego: true # whether to train the ego agent
heldout_eval: true # whether to run a heldout evaluation of the ego agent

# teammate generation settings
algorithm:
  ALG: brdiv
  PARTNER_POP_SIZE: 4
  NUM_EVAL_EPISODES: 20 # used during training
  TRAIN_SEED: 38410
  NUM_ENVS_SP: 16 # 32
  NUM_ENVS_XP: 16 # 32
  # SP weight = 1 + 2*XP weight. 
  # Thus, as XP weight -> 0, SP/(SP+XP) -> 1.
  # If XP weight -> infinity, XP/(SP+XP) -> 1/3, and SP/(SP+XP) -> 2/3.
  XP_LOSS_WEIGHTS: 10 
  ENV_NAME: ${ENV_NAME}
  ENV_KWARGS: ${ENV_KWARGS}
  ROLLOUT_LENGTH: ${ROLLOUT_LENGTH}
  TOTAL_TIMESTEPS: 5e5 # 1e8  # divided among each pair of BR and Conf agents
  LR: 5.e-4
  UPDATE_EPOCHS: 15
  NUM_MINIBATCHES: 16
  NUM_CHECKPOINTS: 5
  GAMMA: 0.99
  GAE_LAMBDA: 0.95
  CLIP_EPS: 0.05
  ENT_COEF: 0.01
  VF_COEF: 1.0
  MAX_GRAD_NORM: 1.0
  ANNEAL_LR: false # TODO: check if we should anneal the learning rate

# ego training settings
ego_train_algorithm:
  TOTAL_TIMESTEPS: 3e5
  NUM_EGO_TRAIN_SEEDS: 1
  NUM_CHECKPOINTS: 5
  ENV_NAME: ${ENV_NAME}
  ENV_KWARGS: ${ENV_KWARGS}
  ROLLOUT_LENGTH: ${ROLLOUT_LENGTH}

# wandb settings
logger: 
  load_dir: brdiv
  project: open-ended-aht
  entity: aht-project
  mode: offline # options: online, offline, disabled
  verbose: true
  log_train_out: false # whether to log the out dictionary
  log_eval_out: false # whether to log the eval metrics

# Local logger
local_logger:
  save_figures: false # unused
  save_train_out: true
  save_eval_out: true