defaults:
  - hydra: hydra_simple
  - _self_

ENV_NAME: lbf
ROLLOUT_LENGTH: 128
ENV_KWARGS: {}
EGO_AGENT_PATH: results/lbf/fcp_s5/2025-04-13_18-42-46/saved_train_run

algorithm:
  ALG: regret_maximizing
  PARTNER_POP_SIZE: 2 # how many regret-maximizing pairs to train
  TRAIN_SEED: 38410
  NUM_ENVS: 16
  ENV_NAME: ${ENV_NAME}
  ENV_KWARGS: ${ENV_KWARGS}
  ROLLOUT_LENGTH: ${ROLLOUT_LENGTH}
  TOTAL_TIMESTEPS: 1.6e7 # 3e6
  LR: 1.e-4
  UPDATE_EPOCHS: 15
  NUM_MINIBATCHES: 4
  NUM_EVAL_EPISODES: 20
  NUM_CHECKPOINTS: 5
  GAMMA: 0.99
  GAE_LAMBDA: 0.95
  CLIP_EPS: 0.05
  ENT_COEF: 0.001 # TODO: check if this should be larger?
  VF_COEF: 1.0
  MAX_GRAD_NORM: 1.0
  ANNEAL_LR: false
  EGO_ACTOR_TYPE: s5
  S5_ACTOR_CRITIC_HIDDEN_DIM: 64
  S5_D_MODEL: 16
  S5_SSM_SIZE: 16
  S5_N_LAYERS: 2
  S5_BLOCKS: 1
  S5_ACTIVATION: full_glu
  S5_DO_NORM: true
  S5_PRENORM: true
  S5_DO_GTRXL_NORM: true

name: regret_maximizing

# wandb settings
logger: 
  load_dir: regret_maximizing
  project: open-ended-aht
  entity: aht-project
  mode: online # options: online, offline, disabled
  verbose: true
  log_train_out: false # whether to log the out dictionary
  log_video: true # whether to log the video

# Local logger
local_logger:
  save_figures: false # currently unused
  save_train_out: false
  save_video: false # whether to save the video locally