defaults:
  - hydra: hydra_simple
  - _self_

ENV_NAME: lbf
ROLLOUT_LENGTH: 128 # should be larger than episode length
ENV_KWARGS: {}

ego_agent:
  path: "results/lbf/ppo_ego_s5/2025-04-13_11-43-33/saved_train_run" # results/lbf/fcp_s5/2025-04-13_18-42-46/saved_train_run
  actor_type: s5
  idx_list: [[0, -1]] # list of idxs to load from checkpoint for evaluation. If null, all checkpoints will be loaded.
  # provide any necessary args to initialize the agent here
  # ex) S5_ACTOR_CRITIC_HIDDEN_DIM: 64 
  # defaults for each actor type will be used if no args are provided

algorithm:
  ALG: regret_maximizing
  PARTNER_POP_SIZE: 1 # how many regret-maximizing pairs to train
  CONF_BR_WEIGHT: 0.5 # conf-ego weight set to 1 - conf-br weight
  TRAIN_SEED: 38410
  NUM_ENVS: 16
  ENV_NAME: ${ENV_NAME}
  ENV_KWARGS: ${ENV_KWARGS}
  ROLLOUT_LENGTH: ${ROLLOUT_LENGTH}
  TOTAL_TIMESTEPS: 1.6e7 # 3e6
  LR: 1.e-4
  UPDATE_EPOCHS: 15
  NUM_MINIBATCHES: 4
  NUM_EVAL_EPISODES: 20
  NUM_CHECKPOINTS: 5
  GAMMA: 0.99
  GAE_LAMBDA: 0.95
  CLIP_EPS: 0.05
  ENT_COEF: 0.001 # TODO: check if this should be larger?
  VF_COEF: 1.0
  MAX_GRAD_NORM: 1.0
  ANNEAL_LR: false

name: regret_maximizing

# wandb settings
logger: 
  load_dir: regret_maximizing
  project: open-ended-aht
  entity: aht-project
  mode: offline # options: online, offline, disabled
  verbose: true
  log_train_out: false # whether to log the out dictionary
  log_video: true # whether to log the video

# Local logger
local_logger:
  save_figures: false # currently unused
  save_train_out: false
  save_video: false # whether to save the video locally