# @package algorithm
# ^ tells hydra to place these value directly under algorithm key
ALG: open_ended_lagrange
TIMESTEPS_PER_ITER_PARTNER: 1.5e6 # per iter of open-ended training, divided between all partner seeds
TIMESTEPS_PER_ITER_EGO: 1e6
NUM_OPEN_ENDED_ITERS: 30
NUM_CHECKPOINTS: 5 # per iter of open-ended training
PARTNER_POP_SIZE: 1
REINIT_CONF: true # whether to reinitialize the confederate policy each iteration of open-ended training
REINIT_BR_TO_BR: true # whether to reinitialize the br policy each iteration of open-ended training
REINIT_BR_TO_EGO: false # whether to reinitialize the br policy to the ego params each iteration of open-ended training
RESET_CONF_BR_TO_EGO_STATES: false # whether to initialize conf-br data collection from conf-ego states
CONF_OBJ_TYPE: per_state_regret
EGO_TEAMMATE: all # choices: [final, all] # whether to use the final conf params as the teammate for the ego agent, or all ckpts
# Weights on SP loss
SP_WEIGHT: 1.0 # weight on conf-br (SP) loss
REGRET_SP_WEIGHT: 1.0 # weight on the SP term in the perstep regret loss
# lagrange args
LAGRANGE_MULTIPLIER_LR: 0.05
LAGRANGE_USE_TARGET_RETURNS: true
NUM_ENVS: 16
LR: 1.e-4
UPDATE_EPOCHS: 15
NUM_MINIBATCHES: 4
GAMMA: 0.99
GAE_LAMBDA: 0.95
CLIP_EPS: 0.05
ENT_COEF: 0.01
VF_COEF: 0.5
MAX_GRAD_NORM: 1.0
ANNEAL_LR: false
